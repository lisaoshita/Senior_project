---
title: "Predicting with Trees, Random Forests, & Model Based Predictions"
author: "Lisa Oshita"
date: "1/10/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Predicting with Trees

* Key ideas
    + iteratively split variables into groups and evaluate "homogeneity" within each group 
    + split again if necessary 
* pros:
    + easy to interpret, better performance in nonlinear settings
* cons: 
    + without pruning/cross-validation can lead to overfitting
    + harder to estimate uncertainty
    + results may be variable
* Basic algorithm
    + start with all variables in one group 
    + find variable/split that best separates outcomes
    + divide data into two groups ("leaves") on that split ("node")
    + within each split, find best variable that separates outcome 
    + continue until groups are too small or sufficiently "pure"
* Measures of impurity
    + Misclassification error: 0 = perfect purity, 0.5 = no purity
    + Gini index: values are same as above
    + Deviance/information gain: 0 = perfect, 1 = none

```{r}
# example with iris data
data(iris)
library(ggplot2)
table(iris$Species)
```

```{r}
# create training/test set
inTrain <- caret::createDataPartition(y = iris$Species, p = 0.7, list = FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
```

```{r}
qplot(Petal.Width, Sepal.Width, colour = Species, data = training)
```

```{r}
modFit <- caret::train(Species ~., method = "rpart", data = training) # predicting with all variables in data
print(modFit$finalModel)
# all flowers with petal < 2.6 belong to species setosa 

# plot tree
plot(modFit$finalModel, uniform = TRUE, main = "Classification Tree")
text(modFit$finalModel, use.n = TRUE, all = TRUE, cex = 0.8)

# predicting new values
predict(modFit, newdata = testing)
```

#### Bagging (bootstrap aggregating)

* Basic idea
    + resample cases, recalculate predictions
    + average or majority vote

```{r}
# using ozone data
library(ElemStatLearn)
data(ozone, package = "ElemStatLearn")
ozone <- ozone[order(ozone$ozone), ] # order data set by outcome 
head(ozone)
```

```{r}
# bagged loess
ll <- matrix(NA, nrow = 10, ncol = 155)
# resampled 10 times, fit loess curve 10 times, then average values
for (i in 1:10) {
  ss <- sample(1:dim(ozone)[1], replace = TRUE)
  ozone0 <- ozone[ss,]
  ozone0 <- ozone0[order(ozone0), ]
  loess0 <- loess(temperature ~ ozone, data = ozone0, span = 0.2)
  ll[i,] <- predict(loess0, newdata = data.frame(ozone = 1:155))
}

plot(ozone$ozone, ozone$temperature, pch = 19, cex = 0.5)
for (i in 1:10){lines(1 : 155, ll[i,], col = "grey", lwd = 2)}
lines(1:155, apply(ll, 2, mean), col = "red", lwd = 2) # average of all loess curves

# can also bag in caret
```




















