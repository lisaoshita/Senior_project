---
title: "Regularized Regression, combining predictors"
author: "Lisa Oshita"
date: "1/15/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Regularized regression 

* fit model, penalize (shrink) large coefficients of some predictors
* pros:
    + can help with bias/variance tradeoff
    + can help with model selection
* cons:
    + may be computationally inefficient
    + doesn't perform as well as random forests and boosting
* model selection approach: split samples
    + no method is better when data/computation time permits it
    + approach: divide into training/test/validation, treat validation as test data, train all competing models on train data and pick best one on validation, to appropriately assess performance on new data apply to test data, re-split and re-perform steps 1-3
    + common problems: limited data, computational complexity
* decomposing expected prediction error
    + = irreducible error + bias^2 + variance
* regularization for regression
    + if betas are unconstrained - they may "explode", susceptible to high variance
    + to control variance - regularize/shrink coefficients - PRSS: penalized form of sum of squares
    + penalty reduces: complexity, variance, respects the structure of the problem
* ridge regression - tuning parameter (lambda)
    + lambda controls size of coefficients
    + controls amount of regularization
    + as lambda appraoches 0, obtain least square solution 
    + as lambda approaches infinity, all betas go to 0
    + choosing parameter - done with cross validation, other techniques, lasso 

#### Combining predictors

* combine classifiers by averaging/voting, classifiers can be different (regression with boost)
* improves accuracy, reduces interpretability 
* boosting, bagging, random forests are variants of this theme

```{r}
# creating training, testing, validation sets
library(ISLR); data(Wage); library(ggplot2); library(caret)
Wage <- subset(Wage, select = -c(logwage))

inBuild <- createDataPartition(y = Wage$wage, p = 0.7, list = FALSE)
validation <- Wage[-inBuild, ]
buildData <- Wage[inBuild, ]

inTrain <- createDataPartition(y = buildData$wage, p = 0.7, list = FALSE)
training <- buildData[inTrain, ]
testing <- buildData[-inTrain, ]
```

```{r}
mod1 <- train(wage~., method = "glm", data = training) # linear model
mod2 <- train(wage~., method = "rf", data = training, trControl = trainControl(method = "cv"), number = 3) 
# random forests

pred1 <- predict(mod1, testing); pred2 <- predict(mod2, testing)
qplot(pred1, pred2, colour = wage, data = testing)
```

```{r}
# model that combines predictors using test set 
predDF <- data.frame(pred1, pred2, wage = testing$wage) 
combModFit <- train(wage~., method = "gam", data = predDF)
combpred <- predict(combModFit, predDF)

# errors
sqrt(sum((pred1 - testing$wage) ** 2)) # performance on test set for model 1
sqrt(sum((pred2 - testing$wage) ** 2)) # performance on test set for model 2
sqrt(sum((combpred - testing$wage) ** 2)) # performance for combined model 

# predicting on validation set 
pred1V <- predict(mod1, validation) # prediction of first and second model on validation data 
pred2V <- predict(mod2, validation)
predVDF <- data.frame(pred1 = pred1V, pred2 = pred2V)
combPredV <- predict(combModFit, predVDF) # predict using combined model 

# errors
sqrt(sum((pred1V - validation$wage) ** 2))
sqrt(sum((pred2V - validation$wage) ** 2))
sqrt(sum((combPredV - validation$wage) ** 2))
```

#### Forecasting

* what's different
    + data is dependent on time
    + specific pattern types: trends, seasonal patterns, cycles
    + subsampling into training/testing can be more complicated 
    + goal: predict one/more observations into the future

```{r}
# forecasting with google data
library(quantmod)
from.dat <- as.Date("01/01/08", format = "%m/%d/%y")
to.dat <- as.Date("12/31/13", format = "%m/%d/%y")

getSymbols("GOOG", src = "google", from = from.dat, to = to.dat)
head(GOOG)
GOOG <- subset(GOOG, select = -c(GOOG.Volume)) # removing column with NA

mGOOG <- to.monthly(GOOG)
googOpen <- Op(mGOOG)
ts1 <- ts(googOpen, frequency = 12) # creating time series object
plot(ts1, xlab = "Years + 1", ylab = "GOOG") # displaying monthly opening prices over period of 7 yrs
```

```{r}
# decompose time series into parts (trends, seasonal patterns, or cyclic)
plot(decompose(ts1), xlab = "Years + 1")
# indicates an upward trend + seasonal pattern + random pattern is present 
```

```{r}
# training/testing
ts1Train <- window(ts1, start = 1, end = 5) # starts at time point 1, ends at 5
ts1Test <- window(ts1, start = 5, end = (7 - 0.01))
```

```{r}
library(forecast)
# simple moving average
plot(ts1Train)
lines(forecast::ma(ts1Train, order = 3), col = "red")

# exponential smoothing
ets1 <- ets(ts1Train, model = "MMM")
fcast <- forecast(ets1)
plot(fcast); lines(ts1Test, col = "red")

accuracy(fcast, ts1Test)
```

#### Unsupervised prediction

* when you don't know labels for prediction
* to build a predictor
    + create clusters, name clusters, build predictors for clusters
    + in new data set, predict clusters

```{r}
data(iris) 
inTrain_iris <- createDataPartition(y = iris$Species, p = 0.7, list = FALSE)
training_iris <- iris[inTrain_iris, ]
testing_iris <- iris[-inTrain_iris, ]

# perform k-means clustering
kMeans1 <- kmeans(subset(training_iris, select = -c(Species)), centers = 3) # ignoring species clusters
training_iris$clusters <- as.factor(kMeans1$cluster)
qplot(Petal.Width, Petal.Length, colour = clusters, data = training_iris)
# compare to real labels 
table(kMeans1$cluster, training_iris$Species)

# build predictor 
modFit <- train(clusters~., data = subset(training_iris, select = -c(Species)), method = "rpart")
table(predict(modFit, training_iris), training_iris$Species)

# apply on test data 
testClusterPred <- predict(modFit, testing_iris)
table(testClusterPred, testing_iris$Species) # indicates that cluster 1 and 3 tend to get mixed up 
```
