---
title: "Boosting Models"
author: "Lisa Oshita"
date: "2/2/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
levels <- data.frame(countries = levels(train$country_destination), num = 1:12)

# function to match number with country
get_countries <- function(predict) {
  predict[predict == 1] <- "AU"; predict[predict == 2] <- "CA"
  predict[predict == 3] <- "DE"; predict[predict == 4] <- "ES"
  predict[predict == 5] <- "FR"; predict[predict == 6] <- "GB"
  predict[predict == 7] <- "IT"; predict[predict == 8] <- "NDF"
  predict[predict == 9] <- "NL"; predict[predict == 10] <- "other"
  predict[predict == 11] <- "PT"; predict[predict == 12] <- "US"
  return(predict)
}
```

#### 5-fold CV of Gradient Boosting 

```{r}
library(gbm)

# function to perform cross-validation
gbm_cv <- function(fold) {
  # set up training and test sets 
  training <- train[fold, ] 
  testing <- train[-fold, ]
  
  # fit gradient boosted model
  fit <- gbm(country_destination ~ ., data = training, 
             distribution = "multinomial", n.trees = 5)
  
  # compute predictions on test set 
  predict <- predict(fit, newdata = testing, n.trees = 5, type = "response")
  # translate predictions to countries
  predict <- apply(predict, 1, which.max) 
  predict <- get_countries(predict)  
  
  # return list containing accuracy + confusion matrix
  return(list(fit, 
              predict,
              sum(testing$country_destination == predict) / nrow(testing),
              table(predict, testing$country_destination))) 
}

# creating test and train sets
folds <- caret::createFolds(train$country_destination, k = 3, list = TRUE, returnTrain = TRUE)

# iterate over each fold and apply cv function 
cv_results <- purrr::map(folds, ~gbm_cv(.))

cv_results # 100% accuracy still 
```

##### XGBoost

* faster than gradient boosting
* only works with numeric vectors
    + one hot encoding for categorical variables
    
##### Booster Parameters

* nrounds: max number of iterations
    + similar to number of trees to grow
    + tune using CV
* eta (range 0-1): controls learning rate
    + lower/slower learning rate - slower computation, must be supported by increase in nrounds
    + typically lies between 0.1 and 0.3
* gamma: controls regularization (prevents overfitting)
    + Higher the value, higher the regularization (default = 0, no regularization)
    + Regularization means penalizing large coefficients that don't improve performance
    + Tune trick: Start with 0, check CV error rate. If train error >>> test error, include gamma. Higher the gamma, lower the difference in train and test CV. (starting point: use gamma=5 and see the performance)
    + brings improvement when you want to use shallow (low max_depth) trees
* max_depth (depth of the tree)
    + larger the depth - more chances of overfitting
    + tune with CV
* min_child_weight
    + if leaf node has minimum sum of instance weight lower than min_child_weight - splitting stops 
    + prevents overfitting
    tune with CV
* subsample
    + number of subsamples supplied to a tree -typically between 0.5 - 0.8
* colsample_bytree
    + controls number of features supplied to a tree (between 0.5 - 0.9)
    
##### Learning parameters

* objective: multi:softmax, multi:softprob
* eval_metric: AUC, mlogloss (error metrics)

```{r}
library(xgboost)

# creating test and train sets
inTrain <- caret::createDataPartition(y = train$country_destination, p = 0.7, list = FALSE)
mtrain <- data.matrix(train[inTrain, ])
mtest <- data.matrix(train[-inTrain, ])
mtrain[,1] <- mtrain[,1] - 1
mtest[,1] <- mtest[,1] - 1

# converting data frame to xgb.DMatrix (recommended with xgboost)
dtrain <- xgb.DMatrix(data = mtrain[,-1], label = mtrain[,1]) 
dtest <- xgb.DMatrix(data = mtest[,-1], label = mtest[,1])

# default parameters
params <- list("objective" = "multi:softprob",
               "num_class" = 12,
               eta = 0.3, 
               gamma = 0, 
               max_depth = 6, 
               min_child_weight = 1, 
               subsample = 1, 
               colsample_bytree = 1)
n_round <- 50
xgb_cv <- xgb.cv(params = params, 
                 data = dtrain,
                 nfold = 5, 
                 nrounds = n_round, 
                 maximize = TRUE, 
                 print_every_n = 10, 
                 early_stop_round = 2, 
                 prediction = TRUE)

# predictions
oof_prediction <- data.frame(xgb_cv$pred) %>% mutate(max_prob = max.col(., ties.method = "last"), label = mtrain[,1] + 1)

# confusion matrix
confusionMatrix(factor(oof_prediction$label), factor(oof_prediction$max_prob), mode = "everything")

# variable importance
var_importance <- xgb.importance(feature_names = colnames(mtrain), model = xgb_cv)
xgb.plot.importance (importance_matrix = mat[1:20]) 

# parameter tuning 

# fit model to all of train data 
# xgb.train function crashes r

```


