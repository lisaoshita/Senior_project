---
title: "Boosting Models"
author: "Lisa Oshita"
date: "2/2/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# function to match number with country
get_countries <- function(predict) {
  predict[predict == 1] <- "AU"; predict[predict == 2] <- "CA"
  predict[predict == 3] <- "DE"; predict[predict == 4] <- "ES"
  predict[predict == 5] <- "FR"; predict[predict == 6] <- "GB"
  predict[predict == 7] <- "IT"; predict[predict == 8] <- "NDF"
  predict[predict == 9] <- "NL"; predict[predict == 10] <- "other"
  predict[predict == 11] <- "PT"; predict[predict == 12] <- "US"
  return(predict)
}
```

```{r}
library(gbm)
library(xgboost)
library(caret)
library(magrittr)
library(dplyr)
```

```{r}
# gradient boosted model
# only using: signup_method, signup_flow, language, affiliate_channel, affiliate_provider, 
# first_affiliate_tracked, signup_app, first_device_type

pract_train <- train[,1:129] # using train data from feature_engineering.rmd 
boostpract <- gbm(country_destination ~., data = pract_train, distribution = "multinomial",
                  n.trees = 5)
boostpract

gbm_predict <- predict(boostpract, newdata = pract_train, n.trees = 5, type = "response")
gbm_predict <- apply(gbm_predict, 1, which.max) 
gbm_predict <- get_countries(gbm_predict)  
table(predict, train_users$country_destination)
```


#### 5-fold CV of Gradient Boosting 

```{r}
# function to perform cross-validation
gbm_cv <- function(fold) {
  # set up training and test sets 
  training <- train[fold, ] 
  testing <- train[-fold, ]
  
  # fit gradient boosted model
  fit <- gbm(country_destination ~ ., data = training, 
             distribution = "multinomial", n.trees = 5)
  
  # compute predictions on test set 
  predict <- predict(fit, newdata = testing, n.trees = 5, type = "response")
  # translate predictions to countries
  predict <- apply(predict, 1, which.max) 
  predict <- get_countries(predict)  
  
  # return list containing accuracy + confusion matrix
  return(list(fit, 
              predict,
              sum(testing$country_destination == predict) / nrow(testing),
              table(predict, testing$country_destination))) 
}

# creating test and train sets
folds <- caret::createFolds(train$country_destination, k = 3, list = TRUE, returnTrain = TRUE)
folds
# iterate over each fold and apply cv function 
cv_results <- purrr::map(folds, ~gbm_cv(.))

cv_results # 100% accuracy still 
```

##### XGBoost

* faster than gradient boosting
* only works with numeric vectors
    + one hot encoding for categorical variables
    
##### Booster Parameters

* nrounds: max number of iterations
    + similar to number of trees to grow
    + tune using CV
* eta (range 0-1): controls learning rate
    + lower/slower learning rate - slower computation, must be supported by increase in nrounds
    + typically lies between 0.1 and 0.3
* gamma: controls regularization (prevents overfitting)
    + Higher the value, higher the regularization (default = 0, no regularization)
    + Regularization means penalizing large coefficients that don't improve performance
    + Tune trick: Start with 0, check CV error rate. If train error >>> test error, include gamma. Higher the gamma, lower the difference in train and test CV. (starting point: use gamma=5 and see the performance)
    + brings improvement when you want to use shallow (low max_depth) trees
* max_depth (depth of the tree)
    + larger the depth - more chances of overfitting
    + tune with CV
* min_child_weight
    + if leaf node has minimum sum of instance weight lower than min_child_weight - splitting stops 
    + prevents overfitting
    tune with CV
* subsample
    + number of subsamples supplied to a tree -typically between 0.5 - 0.8
* colsample_bytree
    + controls number of features supplied to a tree (between 0.5 - 0.9)
    
##### Learning parameters

* objective: multi:softmax, multi:softprob
* eval_metric: AUC, mlogloss (error metrics)

```{r}
# ====================================================================
# ndcg5 metric
# ====================================================================
dcg_at_k <- function (r, k=min(5, length(r)) ) {
  #only coded alternative formulation of DCG (used by kaggle)
  r <- as.vector(r)[1:k]
  sum(( 2^r - 1 )/ log2( 2:(length(r)+1)) )
} 

ndcg_at_k <- function(r, k=min(5, length(r)) ) {
  r <- as.vector(r)[1:k]
  if (sum(r) <= 0) return (0)     # no hits (dcg_max = 0)
  dcg_max = dcg_at_k(sort(r, decreasing=TRUE)[1:k], k)
  return ( dcg_at_k(r, k) / dcg_max )
}

score_predictions <- function(preds, truth) {
  # preds: matrix or data.frame
  # one row for each observation, one column for each prediction.
  # Columns are sorted from left to right descending in order of likelihood.
  # truth: vector
  # one row for each observation.
  preds <- as.matrix(preds)
  truth <- as.vector(truth)
  
  stopifnot( length(truth) == nrow(preds))
  r <- apply( cbind( truth, preds), 1
              , function(x) ifelse( x == x[1], 1, 0))[ -1, ]
  if ( ncol(preds) == 1) r <-  rbind( r, r)  #workaround for 1d matrices
  as.vector( apply(r, 2, ndcg_at_k) )
}

ndcg5 <- function(preds, dtrain) {
  
  labels <- getinfo(dtrain,"label")
  num.class = length(unique(labels))
  pred <- matrix(preds, nrow = num.class)
  top <- t(apply(pred, 2, function(y) order(y)[num.class:(num.class-4)]-1))
  
  x <- ifelse(top==labels,1,0)
  dcg <- function(y) sum((2^y - 1)/log(2:(length(y)+1), base = 2))
  ndcg <- mean(apply(x,1,dcg))
  return(list(metric = "ndcg5", value = ndcg))
}
```


```{r}
# creating test and train sets
inTrain <- caret::createDataPartition(y = train$country_destination, p = 0.7, list = FALSE)
mtrain <- data.matrix(train[inTrain, ])
mtest <- data.matrix(train[-inTrain, ])
mtrain[,1] <- mtrain[,1] - 1
mtest[,1] <- mtest[,1] - 1

# ----------------------------------------------------------------------
# retrying CV
mtrain1 <- mtrain[,-1] # with country_destination removed
train_countries <- mtrain[,1]
dtrain1 <- xgb.DMatrix(data = mtrain1, label = train_countries)

mtest1 <- mtest[,-1]
test_countries <- mtest[,1]
dtest1 <- xgb.DMatrix(data = mtest1, label = test_countries)
# ----------------------------------------------------------------------

# converting data frame to xgb.DMatrix (recommended with xgboost)
dtrain <- xgb.DMatrix(data = mtrain[,-1], label = mtrain[,1]) 
dtest <- xgb.DMatrix(data = mtest[,-1], label = mtest[,1])

# default parameters
params <- list("objective" = "multi:softprob",
               "num_class" = 12,
               eta = 0.3, 
               gamma = 0, 
               max_depth = 6, 
               min_child_weight = 1, 
               subsample = 1, 
               colsample_bytree = 1)
n_round <- 50
xgb_cv <- xgb.cv(params = params, 
                 data = dtrain,
                 nfold = 5, 
                 nrounds = n_round, 
                 maximize = TRUE, 
                 print_every_n = 10, 
                 early_stop_round = 2,
                 feval = ndcg5,
                 prediction = TRUE)

# ----------------------------------------------------------------------
# retrying CV
xgb_cv1 <- xgb.cv(params = params,
                  data = dtrain1, 
                  nfold = 5,
                  nrounds = 5,
                  maximize = TRUE,
                  print_every_n = 2,
                  early_stop_round = 2,
                  prediction = TRUE)
oof_prediction1 <- data.frame(xgb_cv1$pred) %>% mutate(max_prob = max.col(., ties.method = "last"), label = train_countries + 1)
confusionMatrix(factor(oof_prediction1$label), factor(oof_prediction1$max_prob), mode = "everything")
# ----------------------------------------------------------------------

# predictions
oof_prediction <- data.frame(xgb_cv$pred) %>% mutate(max_prob = max.col(., ties.method = "last"), label = mtrain[,1] + 1)

# confusion matrix
confusionMatrix(factor(oof_prediction$label), factor(oof_prediction$max_prob), mode = "everything")

# fit model to all of train data 
# xgb.train function crashes r
xgb_train <- xgboost(params = params,
                     data = dtrain,
                     nrounds = 5,
                     maximize = TRUE, 
                     print_every_n = 10,
                     early_stop_round = 2)

# Predict on test set
test_pred <- predict(xgb_train, newdata = dtest)
test_prediction <- matrix(test_pred, 
                          nrow = 12, 
                          ncol = length(test_pred) / 12) %>% t() %>% data.frame() %>% 
  mutate(label = mtest[,1] + 1, max_prob = max.col(., "last"))

# confusion matrix
confusionMatrix(factor(test_prediction$label),
                factor(test_prediction$max_prob),
                mode = "everything")

# variable importance
importance_m <- xgb.importance(feature_names = colnames(mtrain), model = xgb_train)
importance_m
# gain: improvement in accuracy brought by a feature it branches on 

# plot of variable importance
xgb.plot.importance(importance_m)
```


