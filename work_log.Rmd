---
title: "Work Log"
author: "Lisa Oshita"
date: "1/25/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Winter break/Week 1

* Data science courses on coursera: 
    + reproducible research
    + logistic and poisson regression courses
    + machine learning
* took notes in r markdown on all courses 

#### Week 2

* Continued learning about machine learning algorithms
    + coursera 
    + text readings 

#### Week 3 

* Applied ml algorithms to iris data set 
* Exploratory analysis on Airbnb's data sets (train_users, age_gender)
    + turned date variables into date time vars 
    + graphed variables 
    + looked at missing values 
* applied boosting model with all variables data, achieved 62% accuracy, only predicted US and NDF

#### Week 4 

* Exploratory analysis on all of Airbnb's data sets
    + sampled sessions data 
* feature engineering 
    + pulled apart date variables into month, day, year, weekday, season
    + cleaned age variable + created age_range variable (to mirror age_buckets in age_gender df) 
    + cleaned gender variable
    + one hot encoding of categorical vars 
    + merged age_gender and countries data with train_users 
    + with sessions data: took number of occurences of each unique action type for each user - will continue to do this for all vars in the sessions df 
* fit initial boosting model - 100% accuracy? 

#### Week 5

* 5-fold cross validation of boosted model - tried using function in caret package--R kept crashing 
    + wrote function to perform cross-validation and return accuracy + confusion matrix 
    + obtained 100% accuracy for each fold of cross-validation 
* added variables to training data 
    + count features for each column of sessions data
    + one hot encoding of all categorical features
    + training data - 596 features
* researched gbm and xgboost algorithms
* fit an xgboost algorithm, similar to what Keiichi Kuroyanagi (3rd place winner) did, and based off of xgboost tutorials and readings
    + cross-validation on the training data (100% accuracy) + confusion matrix
    + fit to the full training data and predicted on hold out test data (100% accuracy) + confusion matrix + variable importance
    
#### Week 6

* figured out 100% accuracy issue - adjusted model and data accordingly 
* fit new xgboost - achieved 87% accuracy (obtained with CV as well as with model fit to the full training data)
* view variable importance
* researched ensemble/stacking methods in ML 

