---
title: "Work Log"
author: "Lisa Oshita"
date: "1/25/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Winter break/Week 1

* Data science courses on coursera: 
    + reproducible research
    + logistic and poisson regression courses
    + machine learning
* took notes in r markdown on all courses 

#### Week 2

* Continued learning about machine learning algorithms
    + coursera 
    + text readings 

#### Week 3 

* Applied ml algorithms to iris data set 
* Exploratory analysis on Airbnb's data sets (train_users, age_gender)
    + turned date variables into date time vars 
    + graphed variables 
    + looked at missing values 
* applied boosting model with all variables data, achieved 62% accuracy, only predicted US and NDF

#### Week 4 

* Exploratory analysis on all of Airbnb's data sets
    + sampled sessions data 
* feature engineering 
    + pulled apart date variables into month, day, year, weekday, season
    + cleaned age variable + created age_range variable (to mirror age_buckets in age_gender df) 
    + cleaned gender variable
    + one hot encoding of categorical vars 
    + merged age_gender and countries data with train_users 
    + with sessions data: took number of occurences of each unique action type for each user - will continue to do this for all vars in the sessions df 
* fit initial boosting model - 100% accuracy? 

#### Week 5

* 5-fold cross validation of boosted model - tried using function in caret package--R kept crashing 
    + wrote function to perform cross-validation and return accuracy + confusion matrix 
    + obtained 100% accuracy for each fold of cross-validation 
* added variables to training data 
    + count features for each column of sessions data
    + one hot encoding of all categorical features
    + training data - 596 features
* researched gbm and xgboost algorithms
* fit an xgboost algorithm, similar to what Keiichi Kuroyanagi (3rd place winner) did, and based off of xgboost tutorials and readings
    + cross-validation on the training data (100% accuracy) + confusion matrix
    + fit to the full training data and predicted on hold out test data (100% accuracy) + confusion matrix + variable importance
    
#### Week 6

* figured out 100% accuracy issue - adjusted model and data accordingly 
* fit new xgboost - achieved 87% accuracy (obtained with CV as well as with model fit to the full training data)
* view variable importance
* researched ensemble/stacking methods in ML 

#### Week 7 

* removed variables + refit model to see if achieve better accuracy/computationally faster
    + fit XGBoost to only 250 features - achieved same accuracy + faster, still only predicting a few countries 
* random forest - fit to all variables, with only 10 trees (and then 50) - achieved 87% accuracy both times 
    + assessed out-of-bag performance (can be used instead of CV)
    + fit rf to full training + predicted on test - assessed accuracy 
    + refit random forest to only features with mean decrease gini of greater than 5 (249 features), ran same model, achieved same accuracy 


#### Week 8 

* implement stacked generalization, based on a kaggle tutorial - achieved same accuracy (87%) 
* over/undersampling of training data
    + wrote function to sample rows from each country with or without replacement until n = 10000
    + fit xgboost to new training data, predicted on held out test set (without over/under sampling) 
    + resulted in worse accuracy 
* added more features to training data
    + added difference between date account created and date first booking (days) - split into 4 bins
    + added difference between date first booking and timestamp first active (days) - split into 3 bins
    + added summary statistics of seconds elapsed for each user in sessions data 
    + achieve same accuracy after refitting model to new training data 
* maybe try KNN to predict age + other missing values
* retried over/undersampling - implemented it manually 
    + achieved around the same accuracy, but the model is now able to predict all countries (not well)

