---
title: "ML with iris data"
author: "Lisa Oshita"
date: "1/18/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Preprocessing with PCA 

* some variables may be highly correlated, very similar to each other 
* rather than include all variables, include a summary that most of the information in those variables
* good for linear models

```{r}
library(caret); library(kernlab)
data(iris)
head(iris)
```

```{r}
# creating train/test sets
inTrain <- createDataPartition(y = iris$Species, p = 0.7, list = FALSE) # returns a matrix with all observations for training set (in this case 70% of full data)

train <- iris[inTrain, ] # all exploratory analysis/model building must happen in training set
test <- iris[-inTrain, ]
```

```{r}
corr <- cor(train[,-5]) # calculating correlation for variables in data, with response removed
(M <- abs(corr)) # returns matrix of absolute value of correlations
diag(M) <- 0
M
which(M > 0.8, arr.ind = TRUE) # which variables have correlation > 0.8
# indicates petal.length and sepal.length, petal.width and petal.length are highly correlated 

plot(train$Sepal.Length, train$Petal.Length)
```

* weighted combination of these predictors might be better - pick the combination that captures the most amount of information possible

```{r}
smalltrain <- train[,c(1,3)]
prComp <- prcomp(smalltrain) # performs PCA 

plot(prComp$x[,1], prComp$x[,2])

prComp$rotation 
# PC1 is 0.39 * sepal.length + 0.92 * petal.length
# PC2 is -0.92 * sepal.length + 0.39 * petal.length
# PC1 captures more variation
```

```{r}
# PCA on entire train data 
prcompfull <- prcomp(train[ ,-5])

prcompfull$rotation

library(ggplot2)

typecolor <- (train$Species == "setosa") * 1 + 1
typecolor[train$Species == "virginica"] <- 3
ggplot(as.data.frame(prcompfull$x), aes(x = prcompfull$x[,1], y = prcompfull$x[,2], col = typecolor)) + 
  geom_point() # plot shows separation of species type in PC1
```

```{r}
# PCA with caret package
preProc <- preProcess(train[ , -5], method = "pca", pcaComp = 2) # pcaComp indicates number of PC to compute
irisPC <- predict(preProc, train[ , -5])
irisPC
ggplot(irisPC, aes(x = PC1, y = PC2, col = typecolor)) + 
  geom_point()

# training model 
# pcaModel<- train(irisPC$species ~., method = "glm", data = irisPC) # WILL NOT WORK - glms only predict 2 classes
# testPC <- predict(preProc, testing[, -5]) this will use PC calculated from training data and get new values for test data 
# confusionMatrix <- (test$Species, predict(pcaModel, testPC))
# using pcaModel fit on original data using test PC
# confusion matrix to get accuracy 
```

#### Classification Trees

* Non-linear models
* Monotone transformations may be less important
* Can also be used for a continuous outcome (regression)

```{r}
# EDA
ggplot(training, aes(x = Petal.Width, y = Sepal.Width, col = Species)) + 
  geom_point()
```

```{r}
modFit <- train(Species ~., method = "rpart", data = training)
print(modFit$finalModel)
# 100% of flowers with petal length < 2.45 belong to setosa species 

# dendrogram
library(rpart.plot)
rpart.plot(modFit$finalModel)

# predicting new values
predict(modFit, newdata = test)
```

#### Bagging - bootstrap aggregating

* best for non-linear models 
* for given test observation, record the class predicted by each of the B trees, take majority vote
    + overall prediction is most commonly occuring majority class among the B predictions
* out of bag error (cross-validation for error is not needed)
* RSS (for regression), Gini Index (classification) can give summary of importance of each predictor

```{r}
# example with continuous prediction (bagged loess)
predictors <- data.frame(sepalL = iris$Sepal.Length) # put predictor in df 
petalW <- iris$Petal.Width 
treebag <- caret::bag(predictors, petalW, B = 10, # number of subsamples to take 
                      bagControl = bagControl(fit = ctreeBag$fit, # funct to be applied to fit model each time
                                              predict = ctreeBag$pred,
                                              aggregate = ctreeBag$aggregate)) # ex: average predictions

plot(iris$Sepal.Length, petalW, col = "lightgrey", pch = 19)
points(iris$Sepal.Length, predict(treebag$fits[[1]]$fit, predictors), pch = 19, col = "red") 
points(iris$Sepal.Length, predict(treebag, predictors), pch = 19, col = "blue")
# red dots represent fit from single regression tree
# blue represents average of models 
```

#### Random Forests

* usually top performing algorithms (along with boosting) in ML competitions
* difficult to interpret, but very accurate
* extension of bagging but with every bootstrapped sample, predictors are bootstrapped at each split 
    + predictor are not resampled from all possible predictors, RF forces each split to only consider a subset of all preditors 
* number of trees (B) to fit is not a critical parameter (too many will not lead to overfitting)
    + use value of B with low error 
    + same with bagging

```{r}
rfFit <- train(Species ~., data = train, method = "rf", prox = TRUE)
rfFit
rfFit$finalModel

randomForest::getTree(rfFit$finalModel, k = 2) # view the 2nd tree
# rows correspond to a split

# predicting new values
pred <- predict(rfFit, test)
test$predictRight <- pred == test$Species
table(pred, test$Species)

qplot(Petal.Width, Petal.Length, colour = predictRight, data = test, main = "New Data Predictions")
```

#### Boosting

* boosting libraries in R: gbm (booting with trees), mboost (model based boosting), ada (statistical boosting based on additive logistic regression), gamBoost (boosting generalized additive models)
* similar to bagging, but boosting does not invovle bootstrap resampling and trees are grown sequentially - each tree fit on modified version of original data and grown using information from previously grown trees
    + build model from training data, build second model that attempts to correct errors of the first model
* 3 tuning parameters
    + number of trees (B), boosting can overfit if B is too large, use cross-validation to select B, or optimal M - monitor prediction risk as function of M on validation sample, M that minimizes this risk is taken to be the estimate of M 
    + shrinkage parameter (lambda), controls rate at which boosting learns, small positive number (0.01, 0.001), small lambda can require large B in order to achieve good performance (smaller lambda - more shrinkage, results in larger training risk)
    + number of splits in the tree/interaction depth (d), controls complexity, often d = 1 works well (single split)
* AdaBoost - best for weak learners + binary classification models 
    + sequentially apply weak algorithm to repeatedly modified versions of the data, producing weak classifiers. Predictions from all are combined through weighted majority vote to produce final prediction. Weights give greater influence to more accurate classifiers 
    + over the iterations, observations that are difficult to classify correctly receive increasing influence - each successive classifier forced to concentrate on training observations missed by previous classifiers 

```{r}
# predicting quantitative response
boostMod <- train(Petal.Width ~ ., method = "gbm", data = train, verbose = FALSE)
print(boostMod)

qplot(predict(boostMod, testing), Petal.Width, data = test)

# predicting qualitative response
boostIris <- gbm(Species ~ ., data = train, distribution = "multinomial", 
                 n.trees = 5000, interaction.depth = 4)
summary(boostIris)

# predictions
speciesPredict <- predict(boostIris, newdata = test, n.trees = 5000, type = "response")
pred_class <- data.frame(predictions = apply(speciesPredict, 1, which.max))

pred_class$species <- NA
pred_class$species[pred_class$predictions == 1] <- "setosa"
pred_class$species[pred_class$predictions == 2] <- "versicolor"
pred_class$species[pred_class$predictions == 3] <- "virginica"

test$predictRight2 <- pred_class$species == test$Species
table(pred_class$species, test$Species)
```

```{r}
# example from textbook 
library(gbm)
set.seed(1)
data(Boston, package = "MASS")
trainBoston = sample(1:nrow(Boston), nrow(Boston)/2)

boostBoston <- gbm(medv ~., data = Boston[trainBoston, ], distribution = "gaussian",
                   n.trees = 5000, interaction.depth = 4) # gradient boosted model, default lambda = 0.001
summary(boostBoston)

par(mfrow=c(1,2)) 
plot(boostBoston ,i="rm"); plot(boostBoston ,i="lstat")

# predictions
boostPred <- predict(boostBoston, newdata = Boston[-trainBoston, ], n.trees = 5000)
mean((boostPred - Boston[-trainBoston, ]$medv)^2) # MSE

# boosted model with different lambda 
boostBoston2 <- gbm(medv ~., data = Boston[trainBoston, ], distribution = "gaussian", 
                    n.trees = 5000, interaction.depth = 4, shrinkage = 0.2, verbose = FALSE)
summary(boostBoston2)
# predictions
boostPred2 <- predict(boostBoston2, newdata = Boston[-trainBoston, ], n.trees = 5000)
mean((boostPred2 - Boston[-trainBoston, ]$medv)^2)
```

#### Model based predictions

* linear discriminant analysis, Naive Bayes

```{r}
ldaFit <- train(Species ~ ., data = train, method = "lda")
nbFit <- train(Species ~ ., data = train, method = "nb")

ldaPredict <- predict(ldaFit, test)
nbPredict <- predict(nbFit, test)
table(ldaPredict, nbPredict)
```


#### Feature engineering 

* process of transforming raw data into features that better represent the underlying problem to the predictive models - resulting in improved accuracy 
* feature extraction - automatically reducing dimensionality of observations into a smaller set that can be modelled 
    + PCA, unsupervised clustering methods 
* feature selection - automatically selecting subset that are most useful to a problem 
    + may use rank/scoring method to select (like correlation coefficient)
* feature construction 
    + tabular data - aggregating/combining or decomposing/splitting features to create new features
    + textual data - devising document/content specific indicators... 









