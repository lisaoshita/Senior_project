---
title: "ML with iris data"
author: "Lisa Oshita"
date: "1/18/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Preprocessing with PCA 

* some variables may be highly correlated, very similar to each other 
* rather than include all variables, include a summary that most of the information in those variables
* good for linear models

```{r}
library(caret); library(kernlab)
data(iris)
head(iris)
```

```{r}
# creating train/test sets
inTrain <- createDataPartition(y = iris$Species, p = 0.7, list = FALSE) # returns a matrix with all observations for training set (in this case 70% of full data)

train <- iris[inTrain, ] # all exploratory analysis/model building must happen in training set
test <- iris[-inTrain, ]
```

```{r}
corr <- cor(train[,-5]) # calculating correlation for variables in data, with response removed
(M <- abs(corr)) # returns matrix of absolute value of correlations
diag(M) <- 0
M
which(M > 0.8, arr.ind = TRUE) # which variables have correlation > 0.8
# indicates petal.length and sepal.length, petal.width and petal.length are highly correlated 

plot(train$Sepal.Length, train$Petal.Length)
```

* weighted combination of these predictors might be better - pick the combination that captures the most amount of information possible

```{r}
smalltrain <- train[,c(1,3)]
prComp <- prcomp(smalltrain) # performs PCA 

plot(prComp$x[,1], prComp$x[,2])

prComp$rotation 
# PC1 is 0.39 * sepal.length + 0.92 * petal.length
# PC2 is -0.92 * sepal.length + 0.39 * petal.length
# PC1 captures more variation
```

```{r}
# PCA on entire train data 
prcompfull <- prcomp(train[ ,-5])

prcompfull$rotation

library(ggplot2)

typecolor <- (train$Species == "setosa") * 1 + 1
typecolor[train$Species == "virginica"] <- 3
ggplot(as.data.frame(prcompfull$x), aes(x = prcompfull$x[,1], y = prcompfull$x[,2], col = typecolor)) + 
  geom_point() # plot shows separation of species type in PC1
```

```{r}
# PCA with caret package
preProc <- preProcess(train[ , -5], method = "pca", pcaComp = 2) # pcaComp indicates number of PC to compute
irisPC <- predict(preProc, train[ , -5])
irisPC
ggplot(irisPC, aes(x = PC1, y = PC2, col = typecolor)) + 
  geom_point()

# training model 
# pcaModel<- train(irisPC$species ~., method = "glm", data = irisPC) # WILL NOT WORK - glms only predict 2 classes
# testPC <- predict(preProc, testing[, -5]) this will use PC calculated from training data and get new values for test data 
# confusionMatrix <- (test$Species, predict(pcaModel, testPC))
# using pcaModel fit on original data using test PC
# confusion matrix to get accuracy 
```

#### Classification Trees

* Non-linear models
* Monotone transformations may be less important
* Can also be used for a continuous outcome (regression)

```{r}
# EDA
ggplot(training, aes(x = Petal.Width, y = Sepal.Width, col = Species)) + 
  geom_point()
```

```{r}
modFit <- train(Species ~., method = "rpart", data = training)
print(modFit$finalModel)
# 100% of flowers with petal length < 2.45 belong to setosa species 

# dendrogram
library(rpart.plot)
rpart.plot(modFit$finalModel)

# predicting new values
predict(modFit, newdata = test)
```

#### Bagging - bootstrap aggregating

* best for non-linear models 
* for given test observation, record the class predicted by each of the B trees, take majority vote
    + overall prediction is most commonly occuring majority class among the B predictions

```{r}
# example with continuous prediction (bagged loess)
predictors <- data.frame(sepalL = iris$Sepal.Length) # put predictor in df 
petalW <- iris$Petal.Width 
treebag <- caret::bag(predictors, petalW, B = 10, # number of subsamples to take 
                      bagControl = bagControl(fit = ctreeBag$fit, # funct to be applied to fit model each time
                                              predict = ctreeBag$pred,
                                              aggregate = ctreeBag$aggregate)) # ex: average predictions

plot(iris$Sepal.Length, petalW, col = "lightgrey", pch = 19)
points(iris$Sepal.Length, predict(treebag$fits[[1]]$fit, predictors), pch = 19, col = "red") 
points(iris$Sepal.Length, predict(treebag, predictors), pch = 19, col = "blue")
# red dots represent fit from single regression tree
# blue represents average of models 
```

#### Random Forests

* extension of bagging 
* usually top performing algorithms (along with boosting) in ML competitions
* difficult to interpret, but very accurate

```{r}
rfFit <- train(Species ~., data = train, method = "rf", prox = TRUE)
rfFit
rfFit$finalModel

randomForest::getTree(rfFit$finalModel, k = 2) # view the 2nd tree
# rows correspond to a split

# predicting new values
pred <- predict(rfFit, test)
test$predictRight <- pred == test$Species
table(pred, test$Species)

qplot(Petal.Width, Petal.Length, colour = predictRight, data = test, main = "New Data Predictions")
```

#### Boosting

* boosting libraries in R: gbm (booting with trees), mboost (model based boosting), ada (statistical boosting based on additive logistic regression), gamBoost (boosting generalized additive models)

```{r}
boostMod <- train(Petal.Width ~ ., method = "gbm", data = train, verbose = FALSE)
print(boostMod)

qplot(predict(boostMod, testing), Petal.Width, data = test)
```

#### Model based predictions

* linear discriminant analysis, Naive Bayes

```{r}
ldaFit <- train(Species ~ ., data = train, method = "lda")
nbFit <- train(Species ~ ., data = train, method = "nb")

ldaPredict <- predict(ldaFit, test)
nbPredict <- predict(nbFit, test)
table(ldaPredict, nbPredict)
```












